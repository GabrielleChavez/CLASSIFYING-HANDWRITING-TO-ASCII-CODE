{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Various Machine Learning Models for Handwritten Character Recognition\n",
    "This is our Jupiter Notebook run the code we used to produce results step-by-step.\n",
    "\n",
    "Import necessary helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from testing_models import evaluate_model, test_model_nn\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess import get_data\n",
    "from classifiers import *\n",
    "from nn import *\n",
    "from joblib import dump, load\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from skopt.space import Integer\n",
    "from sklearn import svm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data\n",
    "We used data from the Alpha_Num dataset on kaggle. It contains over 108000 images of handwritten characters. Each image is approximately 28x28 pixels and is in gray-scale. However, before we can train the models we preprocessed them to ensure each image had the same features:\n",
    "* 28x28 pixels: any image less than 28x28 was padded\n",
    "* Gray-Scale\n",
    "\n",
    "For more information on getting data please refer to **preprocess.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_data(\"train\", \"ascii_file_counts.csv\")\n",
    "X_test, y_test = get_data(\"test\", \"ascii_file_counts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Training Traditional Machine Learning Models\n",
    "In this section we will train (or load) and show a brief testing of the following models:\n",
    "* XGBoost\n",
    "* Random Forest\n",
    "* K-Nearest Neighbors\n",
    "\n",
    "The actual results and comparision of models will be done after this section where models are trained (or loaded)\n",
    "\n",
    "## 2.1 Training Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestClassifier()\n",
    "\n",
    "param_space = {\n",
    "    'n_estimators': (10, 500),  # Number of trees\n",
    "    'max_depth': (1, 100),  # Maximum depth\n",
    "    'min_samples_split': (2, 20),  # Minimum number of samples to split\n",
    "    'min_samples_leaf': (1, 20),  # Minimum number of samples to be leaf\n",
    "    'max_features': ['sqrt', 'log2', None],  # Features to consider\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # Measure for split quality\n",
    "    'class_weight': ['balanced', 'balanced_subsample', None],  # Class weights for handling imbalances\n",
    "}\n",
    "\n",
    "bayes_opt = BayesSearchCV(\n",
    "    estimator=RF_model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=30,\n",
    "    cv=5,       # 5-folds\n",
    "    scoring='neg_mean_squared_error',  # Objective function to minimize MSE\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We offer 2 methods to get the Random Forest Model. We trained the model using the code block with the training loop. However, this takes time, so if you want you can directly load the model we provided using the second code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bayes_opt\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      2\u001b[0m RF_model \u001b[38;5;241m=\u001b[39m bayes_opt\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m      3\u001b[0m RF_bayes_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(bayes_opt\u001b[38;5;241m.\u001b[39mcv_results_)\n",
      "File \u001b[1;32mc:\\Users\\Lab account\\anaconda3\\Lib\\site-packages\\skopt\\searchcv.py:542\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[1;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit):\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBayesSearchCV doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support a callable refit, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt define an implicit score to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    540\u001b[0m     )\n\u001b[1;32m--> 542\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, groups\u001b[38;5;241m=\u001b[39mgroups, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_score:\n",
      "File \u001b[1;32mc:\\Users\\Lab account\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Lab account\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1023\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1018\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1019\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1023\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Lab account\\anaconda3\\Lib\\site-packages\\skopt\\searchcv.py:599\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     n_points_adjusted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_iter, n_points)\n\u001b[1;32m--> 599\u001b[0m     optim_result, score_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step(\n\u001b[0;32m    600\u001b[0m         search_space,\n\u001b[0;32m    601\u001b[0m         optimizer,\n\u001b[0;32m    602\u001b[0m         score_name,\n\u001b[0;32m    603\u001b[0m         evaluate_candidates,\n\u001b[0;32m    604\u001b[0m         n_points\u001b[38;5;241m=\u001b[39mn_points_adjusted,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m     n_iter \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n_points\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[1;32mc:\\Users\\Lab account\\anaconda3\\Lib\\site-packages\\skopt\\searchcv.py:453\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[1;34m(self, search_space, optimizer, score_name, evaluate_candidates, n_points)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# make lists into dictionaries\u001b[39;00m\n\u001b[0;32m    451\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m [point_asdict(search_space, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[1;32m--> 453\u001b[0m all_results \u001b[38;5;241m=\u001b[39m evaluate_candidates(params_dict)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# if self.scoring is a callable, we have to wait until here\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;66;03m# to get the score name\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Lab account\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:969\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    965\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    966\u001b[0m         )\n\u001b[0;32m    967\u001b[0m     )\n\u001b[1;32m--> 969\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    970\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    971\u001b[0m         clone(base_estimator),\n\u001b[0;32m    972\u001b[0m         X,\n\u001b[0;32m    973\u001b[0m         y,\n\u001b[0;32m    974\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    975\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    976\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    977\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    978\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    979\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    980\u001b[0m     )\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    982\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    983\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    984\u001b[0m     )\n\u001b[0;32m    985\u001b[0m )\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Lab account\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Lab account\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\Lab account\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lab account\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bayes_opt.fit(X_train, y_train)\n",
    "RF_model = bayes_opt.best_estimator_\n",
    "RF_bayes_df = pd.DataFrame(bayes_opt.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = load('RF.joblib')\n",
    "RF_bayes_df = pd.read_csv('RF_bayes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_list = []\n",
    "acc_list = []\n",
    "prec_list = []\n",
    "recall_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, acc, cm, prec, recall = evaluate_model(y_test, RF_model.predict(X_test))\n",
    "\n",
    "f1_list.append(f1)\n",
    "acc_list.append(acc)\n",
    "prec_list.append(prec)\n",
    "recall_list.append(recall)\n",
    "\n",
    "print(f\"Random Forest: \\n\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Confusion Matrix: \\n{cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XG_model = XGBClassifier(objective='multi:softprob',num_class=93,booster='gbtree',eval_metric= 'mlogloss')\n",
    "\n",
    "param_space = {\n",
    "    'n_estimators': Integer(50, 300),        # Number of trees\n",
    "    'max_depth': Integer(3, 30),             # Depth of each tree     \n",
    "}\n",
    "\n",
    "bayes_opt = BayesSearchCV(\n",
    "    estimator=XG_model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=30,\n",
    "    cv=5,       # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # Objective function: MSE\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_opt.fit(X_train, y_train)\n",
    "XG_model = bayes_opt.best_estimator_\n",
    "XG_bayes_df = pd.DataFrame(bayes_opt.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading trained model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "XG_model = load('xgboost.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: \n",
      "\n",
      "F1 Score: 0.7424123995685852\n",
      "Accuracy: 0.7451540126156821\n",
      "Precision: 0.7455758956660402\n",
      "Recall: 0.7451540126156821\n",
      "Confusion Matrix: \n",
      "[[  28    0    0 ...    0    0    0]\n",
      " [   0   46    0 ...    0    0    0]\n",
      " [   0    0    7 ...    0    0    0]\n",
      " ...\n",
      " [   0    0    0 ...    3    0    0]\n",
      " [   1    0    0 ...    0    2    0]\n",
      " [   0    0    0 ...    0    0 1002]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "f1, acc, cm, prec, recall = evaluate_model(y_test, XG_model.predict(X_test))\n",
    "\n",
    "f1_list.append(f1)\n",
    "acc_list.append(acc)\n",
    "prec_list.append(prec)\n",
    "recall_list.append(recall)\n",
    "\n",
    "print(f\"XGBoost: \\n\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Confusion Matrix: \\n{cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_model = KNeighborsClassifier(weights=\"distance\")\n",
    "\n",
    "param_space = {\n",
    "    'n_neighbors': Integer(1,200)      # Minimum samples per leaf\n",
    "}\n",
    "\n",
    "bayes_opt = BayesSearchCV(\n",
    "    estimator= KNN_model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=20, \n",
    "    cv=5,    \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_opt.fit(X_train, y_train)\n",
    "KNN_model = bayes_opt.best_estimator_\n",
    "KNN_bayes_df = pd.DataFrame(bayes_opt.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading trained model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_model = load('knn_model.pkl')\n",
    "KNN_bayes_df = pd.read_csv('knn_opti.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: \n",
      "\n",
      "F1 Score: 0.7290611882387321\n",
      "Accuracy: 0.7259542336203324\n",
      "Precision: 0.743925974330083\n",
      "Recall: 0.7259542336203324\n",
      "Confusion Matrix: \n",
      "[[  28    0    0 ...    1    0    0]\n",
      " [   0   45    0 ...    0    0    0]\n",
      " [   0    0   12 ...    0    0    0]\n",
      " ...\n",
      " [   0    0    0 ...    4    0    0]\n",
      " [   0    0    0 ...    0    5    0]\n",
      " [   0    0    0 ...    0    0 1007]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "f1, acc, cm, prec, recall = evaluate_model(y_test, KNN_model.predict(X_test))\n",
    "\n",
    "f1_list.append(f1)\n",
    "acc_list.append(acc)\n",
    "prec_list.append(prec)\n",
    "recall_list.append(recall)\n",
    "\n",
    "print(f\"KNN: \\n\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Confusion Matrix: \\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "models = ['Random Forest', 'XGBoost', 'KNN']\n",
    "colors = ['red', 'yellow', 'orange']\n",
    "\n",
    "axes[0, 0].bar(models, f1_list, color=colors, alpha=0.7)\n",
    "axes[0, 0].set_title(\"F1 Scores by Model\")\n",
    "axes[0,0].set_ylim(0.6, 1) \n",
    "\n",
    "axes[0, 1].bar(models, acc_list, color=colors, alpha=0.7)\n",
    "axes[0, 1].set_title(\"Accuracy by Model\")\n",
    "axes[0, 1].set_ylim(0.6, 1) \n",
    "\n",
    "\n",
    "axes[1, 0].bar(models, prec_list, color=colors, alpha=0.7)\n",
    "axes[1, 0].set_title(\"Precision by Model\")\n",
    "axes[1, 0].set_ylim(0.6, 1) \n",
    "\n",
    "\n",
    "axes[1, 1].bar(models, recall_list, color=colors, alpha=0.7)\n",
    "axes[1, 1].set_title(\"Recall by Model\")\n",
    "axes[1, 1].set_ylim(0.6, 1) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training Neural Network Models\n",
    "In this section we will train (or load) and show a brief testing of the following models:\n",
    "* Feed Forward Neural Network\n",
    "* CNN (Convolutional Neural Network )\n",
    "* CNN + LSTM (CNN with an LSTM (Long-Short Term Memort) layer)\n",
    "* Transformer (With CNN features)\n",
    "\n",
    "The details of each model and the PyTorch implementation as well as the training loop details can be found in the **nn.py** file.\n",
    "\n",
    "## Initializing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "num_classes = 93\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "dataset = AlphaNumDataset(csv_dir=\"ascii_file_counts.csv\", data_dir=\"train\")\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataset_test = AlphaNumDataset(csv_dir=\"ascii_file_counts.csv\", data_dir=\"test\")\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = AlphaNumDataset(csv_dir=\"ascii_file_counts.csv\", data_dir=\"validation\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "f1_list_nn = []\n",
    "acc_list_nn = []\n",
    "prec_list_nn = []\n",
    "recall_list_nn = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FF_model = FeedForwardNN(input_size=input_size, num_classes=num_classes, hidden_size=288).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(FF_model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = .2, patience =10)\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(num_epochs, data_loader, val_loader, device, FF_model, criterion, optimizer, scheduler, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FF_model = load('FeedForward.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing + Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, acc, prec, recall = test_model_nn(ff_model, data_loader_test, device, criterion)\n",
    "\n",
    "f1_list_nn.append(f1)\n",
    "acc_list_nn.append(acc)\n",
    "prec_list_nn.append(prec)\n",
    "recall_list_nn.append(recall)\n",
    "\n",
    "print(f\"Feed Forward Neural Network: \\n\")\n",
    "print(f\"F1 Score (Weighted Among Classes): {f1}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision (Weighted Among Classes): {prec}\")\n",
    "print(f\"Recalln (Weighted Among Classes): {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CNN_model = CNN(input_size=input_size, num_classes=num_classes, hidden_size=288).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(FF_model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = .2, patience =10)\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(num_epochs, data_loader, val_loader, device, CNN_model, criterion, optimizer, scheduler, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model = load('cnn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing + Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1,acc,prec,recall = test_model_nn(CNN_model,data_loader_test, device, criterion)\n",
    "\n",
    "f1_list_nn.append(f1)\n",
    "acc_list_nn.append(acc)\n",
    "prec_list_nn.append(prec)\n",
    "recall_list_nn.append(recall)\n",
    "\n",
    "print(f\"Feed Forward Neural Network: \\n\")\n",
    "print(f\"F1 Score (Weighted Among Classes): {f1}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision (Weighted Among Classes): {prec}\")\n",
    "print(f\"Recalln (Weighted Among Classes): {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer_model = CNNTransformer(n_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for classification tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = .2, patience =10)  # Reduce LR by 10x every 5 epochs\n",
    "best_val_loss = float('inf')\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(num_epochs, data_loader, val_loader, device, transformer_model, criterion, optimizer, scheduler, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = load('transformer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing + Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1,acc,prec,recall = test_model_nn(CNN_model,data_loader_test, device, criterion)\n",
    "\n",
    "f1_list_nn.append(f1)\n",
    "acc_list_nn.append(acc)\n",
    "prec_list_nn.append(prec)\n",
    "recall_list_nn.append(recall)\n",
    "\n",
    "print(f\"Feed Forward Neural Network: \\n\")\n",
    "print(f\"F1 Score (Weighted Among Classes): {f1}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision (Weighted Among Classes): {prec}\")\n",
    "print(f\"Recalln (Weighted Among Classes): {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "models = ['Feed Forward', 'CNN', 'Transformer']\n",
    "colors = ['blue', 'green', 'purple']\n",
    "\n",
    "axes[0, 0].bar(models, f1_list_nn, color=colors, alpha=0.7)\n",
    "axes[0, 0].set_title(\"F1 Scores by Model\")\n",
    "axes[0,0].set_ylim(0.6, 1) \n",
    "\n",
    "axes[0, 1].bar(models, acc_list_nn, color=colors, alpha=0.7)\n",
    "axes[0, 1].set_title(\"Accuracy by Model\")\n",
    "axes[0, 1].set_ylim(0.6, 1) \n",
    "\n",
    "\n",
    "axes[1, 0].bar(models, prec_list_nn, color=colors, alpha=0.7)\n",
    "axes[1, 0].set_title(\"Precision by Model\")\n",
    "axes[1, 0].set_ylim(0.6, 1) \n",
    "\n",
    "\n",
    "axes[1, 1].bar(models, recall_list_nn, color=colors, alpha=0.7)\n",
    "axes[1, 1].set_title(\"Recall by Model\")\n",
    "axes[1, 1].set_ylim(0.6, 1) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m ff_runtime, cnn_runtime, transformer_runtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6.38\u001b[39m, \u001b[38;5;241m7.72\u001b[39m, \u001b[38;5;241m49.12\u001b[39m\n\u001b[0;32m      4\u001b[0m runtimes \u001b[38;5;241m=\u001b[39m [rf_runtime, xgboost_runtime, knn_runtime, ff_runtime, cnn_runtime, transformer_runtime]\n\u001b[1;32m----> 5\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m acc_list \u001b[38;5;241m+\u001b[39m acc_list_nn\n\u001b[0;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRF\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKNN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeedForward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(runtimes)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'acc_list' is not defined"
     ]
    }
   ],
   "source": [
    "rf_runtime, xgboost_runtime, knn_runtime = 1.83, 25.63, .66\n",
    "ff_runtime, cnn_runtime, transformer_runtime = 6.38, 7.72, 49.12\n",
    "\n",
    "runtimes = [rf_runtime, xgboost_runtime, knn_runtime, ff_runtime, cnn_runtime, transformer_runtime]\n",
    "accuracies = acc_list + acc_list_nn\n",
    "labels = ['RF','XGBoost', 'KNN', 'FeedForward', 'CNN', 'Transformer']\n",
    "\n",
    "for i in range(len(runtimes)):\n",
    "    plt.scatter(runtimes[i], accuracies[i], label=labels[i])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
